# Context Limit Bug Fix - Summary

## ‚úÖ Changes Made

### 1. Removed `withSupermemory()` wrapper from `modelProvider.ts`

**Changed:**
- ‚ùå Removed import: `import { withSupermemory } from '@supermemory/tools/ai-sdk';`
- ‚ùå Removed wrapper call: `model = withSupermemory(model, userId, { mode: 'full' });`
- ‚úÖ Changed `let model` to `const model` (no longer mutated)
- ‚úÖ Updated comments to clarify user scoping via `x-sm-user-id` header

**Before:**
```typescript
let model = infiniteChatProvider(claudeModel);
model = withSupermemory(model, userId, { mode: 'full' });
```

**After:**
```typescript
const model = infiniteChatProvider(claudeModel);
```

### 2. Added explicit `maxTokens` to `streamConfig.ts`

**Changed:**
- ‚úÖ Added `maxTokens: 4096` to prevent AI SDK from using default 64000

**Before:**
```typescript
return {
  model,
  messages: convertToModelMessages(processedMessages),
  system: systemPrompt,
  temperature: 0.7,
  // NO maxTokens parameter
  tools,
  // ...
};
```

**After:**
```typescript
return {
  model,
  messages: convertToModelMessages(processedMessages),
  system: systemPrompt,
  temperature: 0.7,
  maxTokens: 4096, // Explicit output limit to prevent exceeding context window
  tools,
  // ...
};
```

### 3. Updated comment in `supermemory.ts`

**Changed:**
- Updated outdated reference to `withSupermemory middleware`
- Now correctly references `x-sm-user-id` header approach

## üéØ What This Fixes

### The Bug
Users were getting this error even when using Supermemory Infinite Chat:
```
Error: input length and max_tokens exceed context limit: 154996 + 64000 > 200000
```

### Root Cause
The `withSupermemory()` wrapper was:
1. Creating a new model instance that bypassed the Supermemory proxy baseURL
2. Causing requests to go directly to Anthropic instead of through the compression layer
3. Combined with no explicit `maxTokens` (defaulting to 64000), this exceeded the context limit

### Why CI Test Passed
The CI test (`supermemory-infinite-chat.test.ts`) did NOT use `withSupermemory()`, so it correctly tested the Infinite Chat proxy in isolation.

## üîç What You Keep

All functionality is preserved:

‚úÖ **Supermemory Infinite Chat proxy** - Still active via baseURL
‚úÖ **User scoping** - Via `x-sm-user-id` header
‚úÖ **Conversation scoping** - Via `x-sm-conversation-id` header  
‚úÖ **Memory tools** - `addMemory` still available via `createSupermemoryTools()`
‚úÖ **Context compression** - Handled by Supermemory proxy automatically

## üìä Files Changed

1. `apps/server/src/routes/chat/config/modelProvider.ts` - Removed `withSupermemory` usage
2. `apps/server/src/routes/chat/config/streamConfig.ts` - Added explicit `maxTokens`
3. `apps/server/src/routes/chat/tools/supermemory.ts` - Updated comment

## üß™ Testing Recommendations

1. ‚úÖ Run existing CI test: `bun test apps/server/src/lib/__tests__/supermemory-infinite-chat.test.ts`
2. ‚úÖ Test with long conversations (150k+ tokens)
3. ‚úÖ Verify `addMemory` tool still works
4. ‚úÖ Check that user context is properly scoped

## üìù Technical Details

### How Context Limiting Works Now

**Before (BROKEN):**
- Input: 154,996 tokens
- Max output: 64,000 tokens (default)
- Total: 218,996 tokens > 200,000 limit ‚ùå

**After (FIXED):**
- Input: Unlimited (compressed by Supermemory proxy)
- Max output: 4,096 tokens (explicit)
- Supermemory handles compression transparently ‚úÖ

### The Supermemory Headers

```typescript
headers: {
  'x-supermemory-api-key': supermemoryApiKey,      // Authenticates with Supermemory
  'x-sm-conversation-id': conversationId,          // Scopes context to conversation
  'x-sm-user-id': userId,                          // Scopes memories to user
}
```

These headers tell the Supermemory proxy to:
- Apply intelligent context compression
- Retrieve relevant user memories
- Maintain conversation continuity

## üéì Lessons Learned

1. **CI tests must match production code paths exactly** - The test worked because it didn't use the problematic wrapper
2. **Always set explicit `maxTokens`** - Relying on SDK defaults can cause unexpected behavior
3. **Middleware/wrappers can break proxy configurations** - Be cautious when wrapping models with custom baseURLs
4. **Extended thinking affects token budgets** - The thinking budget reduces available input space

## üöÄ Next Steps

This fix should resolve the context limit error immediately. If issues persist, possible next debugging steps:

1. Check if Supermemory proxy is actually receiving requests (check logs)
2. Verify API keys are correct
3. Confirm the baseURL is not being overridden elsewhere
4. Test with Supermemory team if the wrapper is meant to work with custom baseURLs

---

**Fix Date:** 2025-11-07  
**Branch:** cursor/debug-chat-context-limit-error-eefa
